## Question: Distributed Log Aggregation and Querying

**Problem Description:**

You are tasked with designing and implementing a system for aggregating and querying logs generated by a distributed system. This system consists of numerous microservices, each producing a stream of log entries. These entries need to be collected, indexed, and made available for querying in real-time or near real-time.

Each log entry is a JSON object containing the following fields:

*   `timestamp`: (integer) Unix timestamp in milliseconds representing the time the log entry was generated.
*   `service_name`: (string) The name of the microservice that generated the log.
*   `log_level`: (string) The severity level of the log entry (e.g., "INFO", "WARN", "ERROR").
*   `message`: (string) The actual log message.
*   `trace_id`: (string, optional) A unique identifier for a distributed transaction, allowing correlation of log entries across different services. This field might be missing.
*   `metadata`: (dictionary, optional) Additional key-value pairs providing context for the log entry. This field might be missing and the keys and values can be any valid JSON types.

Your system should implement the following functionality:

1.  **Log Ingestion:** Accept log entries from multiple services concurrently.  The system should be designed to handle a high volume of incoming log data (e.g., thousands of log entries per second). You can assume logs are sent via HTTP POST requests to a specific endpoint.
2.  **Indexing:** Index the ingested logs to enable efficient querying. Consider which fields to index to support a variety of query types.
3.  **Querying:** Provide an API endpoint that allows users to query the logs based on various criteria. The query API should support the following filters:
    *   `timestamp_range`: (tuple of integers) Filter logs within a given timestamp range (inclusive).
    *   `service_name`: (string) Filter logs from a specific service.
    *   `log_level`: (list of strings) Filter logs with any of the specified log levels.
    *   `message_contains`: (string) Filter logs whose message contains the given substring (case-insensitive).
    *   `trace_id`: (string) Filter logs with a specific trace ID.
    *   `metadata_filter`: (dictionary) Filter logs where the metadata contains the given key-value pairs. If a key exists in the log entry's metadata, its value must match the value in the filter. If a key does not exist in log entry's metadata and the value in the filter is `null`, it should still return that log entry.
4.  **Pagination:** The query API should support pagination to handle large result sets.  Implement `page` and `page_size` parameters to control the returned results.
5.  **Concurrency and Scalability:** Your system should be able to handle concurrent requests and scale horizontally to accommodate increasing data volumes.

**Constraints:**

*   **Memory Limit:** The system should be designed to avoid loading the entire log dataset into memory. Efficient use of disk storage and data structures is crucial.
*   **Query Performance:**  Query responses should be reasonably fast (e.g., within a few seconds) even for large datasets.
*   **Real-time/Near Real-time:** The system should strive to make newly ingested logs available for querying as quickly as possible.
*   **Data Durability:**  The ingested logs must be stored durably to prevent data loss.
*   **Resource Constraints:** You may need to work within resource constraints such as limited CPU, memory, and disk space.

**Bonus Challenges:**

*   Implement support for aggregation queries (e.g., count the number of log entries for each service within a given time range).
*   Implement a mechanism for automatically archiving older logs to reduce storage costs.
*   Design a fault-tolerant architecture that can withstand failures of individual components.
*   Implement optimizations for specific query patterns based on observed usage.
*   Implement full text search capabilities for the `message` field.

Good luck!
